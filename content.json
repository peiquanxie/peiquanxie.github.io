{"meta":{"title":"PQ.XIE BLOG","subtitle":"","description":"个人博客","author":"PQ.XIE","url":"http://example.com","root":"/"},"pages":[{"title":"","date":"2023-07-22T02:40:44.436Z","updated":"2023-07-22T02:40:44.436Z","comments":true,"path":"404.html","permalink":"http://example.com/404.html","excerpt":"","text":"404 很抱歉，您访问的页面不存在 可能是输入地址有误或该地址已被删除"},{"title":"","date":"2023-07-23T13:55:59.625Z","updated":"2023-07-23T13:55:59.625Z","comments":true,"path":"about/index.html","permalink":"http://example.com/about/index.html","excerpt":"","text":"联系方式邮箱：&#x32;&#49;&#x32;&#x39;&#x35;&#x31;&#54;&#x39;&#x32;&#64;&#x71;&#x71;&#46;&#99;&#111;&#x6d; 版权声明站点内的所有原创内容（包括但不限于文章、图像等）除特别声明外均采用知识共享署名-非商业性使用-相同方式共享 4.0 国际许可协议，任何人都可以自由传播，但不得用于商用且必须署名并以相同方式分享。本站部分内容转载于网络，有出处的已在文中署名作者并附加原文链接，出处已不可寻的皆已标注来源于网络。若您认为本站点有部分内容侵犯了您的权益，请在电邮告知，我将认真处理。"},{"title":"所有分类","date":"2023-07-22T02:37:26.591Z","updated":"2023-07-22T02:37:26.591Z","comments":true,"path":"categories/index.html","permalink":"http://example.com/categories/index.html","excerpt":"","text":""},{"title":"友情链接","date":"2023-07-23T13:49:37.580Z","updated":"2023-07-23T13:49:37.580Z","comments":true,"path":"friends/index.html","permalink":"http://example.com/friends/index.html","excerpt":"","text":""},{"title":"所有标签","date":"2023-07-22T02:37:59.687Z","updated":"2023-07-22T02:37:59.687Z","comments":true,"path":"tags/index.html","permalink":"http://example.com/tags/index.html","excerpt":"","text":""}],"posts":[{"title":"","slug":"pytorch基础/5-模型调参","date":"2023-07-26T09:18:44.711Z","updated":"2023-07-26T09:18:44.763Z","comments":true,"path":"2023/07/26/pytorch基础/5-模型调参/","link":"","permalink":"http://example.com/2023/07/26/pytorch%E5%9F%BA%E7%A1%80/5-%E6%A8%A1%E5%9E%8B%E8%B0%83%E5%8F%82/","excerpt":"","text":"title: 5-模型调参comments: true #是否可评论toc: true #是否显示文章目录categories: “Pytorch基础” #分类tags: “Pytorch基础” 1 超参数超参数是可调整的参数，让你控制模型优化过程。不同的超参数值会影响模型的训练和收敛率。 常用的超参数如下： epoch : 在整个数据集上迭代的次数。 batch : 每一次更新参数（反向传播）所用到的数据样本数。 学习率 : 每个batch 更新模型参数的步进幅度。 123learning_rate = 1e-3batch_size = 64epochs = 5 2 损失函数当遇到一些训练数据时，我们未经训练的网络很可能不会给出正确的答案。损失函数衡量的是获得的结果与目标值的不相似程度，它是我们在训练期间想要最小化的损失函数。为了计算损失，我们使用给定数据样本的输入进行预测，并与真实数据标签值进行比较。 常见的损失函数包括用于回归任务的nn.MSELoss（均方误差）和用于分类的nn.NLLLoss（负对数似然）。nn.CrossEntropyLoss结合了nn.LogSoftmax和nn.NLLLoss。 我们将模型的输出对数传递给 nn.CrossEntropyLoss，它将对对数进行标准化处理并计算预测误差。 12# Initialize the loss functionloss_fn = nn.CrossEntropyLoss() 3 优化器优化是在每个训练步骤中调整模型参数以减少模型误差的过程。优化算法定义了这个过程是如何进行的（在这个例子中，我们使用随机梯度下降法）。所有的优化逻辑都被封装在优化器对象中。在这里，我们使用SGD优化器；此外，PyTorch中还有许多不同的优化器，如Adam和RMSProp，它们对不同类型的模型和数据有更好的效果。 我们通过注册需要训练的模型参数来初始化优化器，并传入学习率超参数。 1optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate) 4 调参过程我们定义了train_loop和test_loop，train_loop负责循环我们的优化代码，test_loop负责根据测试数据评估模型的性能。 12345678910111213141516171819202122232425262728293031def train_loop(dataloader, model, loss_fn, optimizer): size = len(dataloader.dataset) for batch, (X, y) in enumerate(dataloader): # Compute prediction and loss pred = model(X) loss = loss_fn(pred, y) # Backpropagation optimizer.zero_grad() loss.backward() optimizer.step() if batch % 100 == 0: loss, current = loss.item(), batch * len(X) print(f&quot;loss: &#123;loss:&gt;7f&#125; [&#123;current:&gt;5d&#125;/&#123;size:&gt;5d&#125;]&quot;)def test_loop(dataloader, model, loss_fn): size = len(dataloader.dataset) num_batches = len(dataloader) test_loss, correct = 0, 0 with torch.no_grad(): for X, y in dataloader: pred = model(X) test_loss += loss_fn(pred, y).item() correct += (pred.argmax(1) == y).type(torch.float).sum().item() test_loss /= num_batches correct /= size print(f&quot;Test Error: \\n Accuracy: &#123;(100*correct):&gt;0.1f&#125;%, Avg loss: &#123;test_loss:&gt;8f&#125; \\n&quot;) 123456789loss_fn = nn.CrossEntropyLoss()optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)epochs = 10for t in range(epochs): print(f&quot;Epoch &#123;t+1&#125;\\n-------------------------------&quot;) train_loop(train_dataloader, model, loss_fn, optimizer) test_loop(test_dataloader, model, loss_fn)print(&quot;Done!&quot;) 请确保在推理前调用model.eval()方法，以将dropout和batch normalization层设置为eval模式。如果不这样做，将产生不一致的推理结果。 5 保存&amp;加载5.1 保存状态字典123torch.save(model.state_dict(), &#x27;model_weights.pth&#x27;)model = Model_Net() # 需要实例化训练时相同的类model.load_state_dict(torch.load(&#x27;model_weights.pth&#x27;)) 5.2 保存整个模型类123# 这种方法在序列化模型时使用Python的pickle模块，所以它在加载模型时，依赖于实际的可用的类定义。torch.save(model, &#x27;model.pth&#x27;)model = torch.load(&#x27;model.pth&#x27;) 5.3 导出ONNX模型123# 由于PyTorch执行图的动态性质，导出过程必须遍历执行图以产生持久的ONNX模型。出于这个原因，应该向导出程序传递一个适当大小的测试变量。input_image = torch.zeros((1,3,224,224))onnx.export(model, input_image, &#x27;model.onnx&#x27;) 5.4 导出JIT模型该种方法加载模型时无需 模型类，导出 JIT 模型的方式有两种：trace 和 script。 采用 torch.jit.trace 的方式来导出 JIT 模型，这种方式会根据一个输入将模型跑一遍，然后记录下执行过程。这种方式的问题在于对于有分支判断的模型不能很好的应对，因为一个输入不能覆盖到所有的分支。但是在我们 ResNet50 模型中不会遇到分支判断，因此这里是合适的。 12345# trace 方法保存example_input = torch.rand(1, 3, 224, 224)jit_model = torch.jit.trace(model, example_input)torch.jit.save(jit_model, &#x27;resnet50_jit.pth&#x27;)module = torch.jit.load(&#x27;resnet50_jit.pth&#x27;) 如果模型有 if else 等分支语句, 应该用script方法保存模型。 1234# script 方法保存script_module = torch.jit.script(model) torch.jit.save(script_module, &#x27;model.pth&#x27;)module = torch.jit.load(&#x27;model.pth&#x27;)","categories":[],"tags":[]},{"title":"4-模型搭建","slug":"pytorch基础/4-模型搭建","date":"2023-07-26T09:18:44.419Z","updated":"2023-07-26T09:18:44.467Z","comments":true,"path":"2023/07/26/pytorch基础/4-模型搭建/","link":"","permalink":"http://example.com/2023/07/26/pytorch%E5%9F%BA%E7%A1%80/4-%E6%A8%A1%E5%9E%8B%E6%90%AD%E5%BB%BA/","excerpt":"","text":"1 神经网络搭建1.1 通过 forward 构建我们可以通过继承 nn.Module，构建我们自己的类来定义我们的神经网络。其中，我们在__init__方法中实现各个子Module的初始化，并在forward 方法中组织这些子Module，形成神经网络。 12345678910111213141516171819class NeuralNetwork(nn.Module): def __init__(self): super(NeuralNetwork, self).__init__() self.flatten = nn.Flatten() self.linear_relu_stack = nn.Sequential( nn.Linear(28*28, 512), nn.ReLU(), nn.Linear(512, 512), nn.ReLU(), nn.Linear(512, 10), ) def forward(self, x): x = self.flatten(x) logits = self.linear_relu_stack(x) return logitsmodel = NeuralNetwork().to(device)logits = model(input_image) 1.2 通过 Sequential 构建nn.Sequential是一个有序模块的容器。数据以定义的顺序通过所有的模块。你可以使用 序列容器来组建一个快速的网络。 1234567seq_modules = nn.Sequential( flatten, layer1, nn.ReLU(), nn.Linear(20, 10))logits = seq_modules(input_image) 2 模型参数神经网络中的许多层都是参数化的，也就是说，层相关的权重和偏置在训练中被优化。nn.Module的子类会自动跟踪你的模型对象中定义的所有字段，并使用你的模型的 parameters() 或 named_parameters() 方法访问所有参数。 在这个例子中，我们遍历每个参数，并打印其大小和预览其值。 1234print(&quot;Model structure: &quot;, model, &quot;\\n\\n&quot;)for name, param in model.named_parameters(): print(f&quot;Layer: &#123;name&#125; | Size: &#123;param.size()&#125; | Values : &#123;param[:2]&#125; \\n&quot;) 3 反向传播在训练神经网络时，最常使用的算法是反向传播算法。在这种算法中，参数（模型权重）是根据损失函数相对于给定参数的梯度来调整的。 3.1 自动梯度计算为了计算这些梯度，PyTorch有一个内置的微分引擎，叫做torch.autograd。它支持对任何计算图的梯度进行自动计算。考虑最简单的单层神经网络，输入x，参数w和b，以及一些损失函数。 在这个网络中，w和b是参数，我们需要进行优化。因此，我们需要能够计算损失值相对于这些变量的梯度（梯度只对于模型参数有意义）。为了做到这一点，我们设置了这些tensor的 requires_grad 属性。它可以在PyTorch中以如下方式定义： 12345678910111213# 正向传播import torchx = torch.ones(5) # input tensory = torch.zeros(3) # expected outputw = torch.randn(5, 3, requires_grad=True) # 需要更新的weightb = torch.randn(3, requires_grad=True) # 需要更新的biasz = torch.matmul(x, w)+bloss = torch.nn.functional.binary_cross_entropy_with_logits(z, y)#反向传播loss.backward() #该步骤包含对梯度的自动计算，梯度值通过以下查看。print(w.grad)print(b.grad) 3.2 autograd的机制首先了解tensor有哪些属性： data : 被包装的张量 grad : 存储data的梯度 grad_fn : 创建 Tensor的 Function，是自动求导的关键 requires_grad：指示是否需要梯度 is_leaf : 指示是否是叶子结点 dtype：张量的数据类型 shape：张量的形状，如(64，3，224，224) device：张量所在设备，GPU&#x2F;CPU Tensor和Function互相结合就可以构建一个记录有整个计算过程的有向无环图(Directed Acyclic Graph，DAG)。每个Tensor都有一个.grad_fn属性，该属性即创建该Tensor的Function。 DAG的节点是Function对象，边表示数据依赖，从输出指向输入。 每当对Tensor施加一个运算的时候，就会产生一个Function对象，它产生运算的结果，记录运算的发生，并且记录运算的输入。Tensor使用.grad_fn属性记录这个计算图的入口。反向传播过程中，autograd引擎会按照逆序，通过Function的backward依次计算梯度。 注意事项 （1）梯度不自动清零，如果不清零梯度会累加，所以需要在每次梯度后人为清零。（2）依赖于叶子结点的结点，requires_grad默认为True。（3）叶子结点不可执行in-place，因为其他节点在计算梯度时需要用到叶子节点，所以叶子地址中的值不得改变否则会是其他节点求梯度时出错。所以叶子节点不能进行原位计算。（4）在 y.backward()时，如果 y 是标量量，则不需要为backward()传⼊入任何参数；否则，需要传⼊一个与y同形的Tensor。 （5）只能获得计算图的叶子节点的grad属性，这些节点的requires_grad属性设置为True。对于图中的所有其他节点，梯度将不可用。 （6）只能在一个给定的图上使用一次backward来进行梯度计算。如果需要在同一个图上进行多次backward调用，需要在backward调用中传递 retain_graph&#x3D;True。 （7）在PyTorch中，DAG是动态的。需要注意的是，图是从头开始重新创建的；在每次调用.backward()后，autograd开始填充一个新的图。这正是允许你在模型中使用控制流语句的原因；如果需要，你可以在每次迭代时改变形状、大小和操作。 3.3 禁用梯度跟踪123456789101112# 方法一：利用torch.no_grad()块包围正向传播代码with torch.no_grad(): z = torch.matmul(x, w)+b# 方法二：利用detach方法z = torch.matmul(x, w)+bz_det = z.detach()# 方法三：设置requires_gradfor name, param in model.named_parameters(): param.requires_grad = False","categories":[{"name":"Pytorch基础","slug":"Pytorch基础","permalink":"http://example.com/categories/Pytorch%E5%9F%BA%E7%A1%80/"}],"tags":[{"name":"Pytorch基础","slug":"Pytorch基础","permalink":"http://example.com/tags/Pytorch%E5%9F%BA%E7%A1%80/"}]},{"title":"3-数据准备","slug":"pytorch基础/3-数据准备","date":"2023-07-25T06:38:09.422Z","updated":"2023-07-25T06:38:09.474Z","comments":true,"path":"2023/07/25/pytorch基础/3-数据准备/","link":"","permalink":"http://example.com/2023/07/25/pytorch%E5%9F%BA%E7%A1%80/3-%E6%95%B0%E6%8D%AE%E5%87%86%E5%A4%87/","excerpt":"","text":"1 Dataset1.1 图像（path+label形式）12345678910111213141516171819202122232425import osimport pandas as pdfrom torchvision.io import read_imagefrom torchvision import datasetsfrom torchvision import transformsclass CustomImageDataset(Dataset): def __init__(self, annotations_file, img_dir, transform=None, target_transform=None): self.img_labels = pd.read_csv(annotations_file) self.img_dir = img_dir self.transform = transform self.target_transform = target_transform def __len__(self): return len(self.img_labels) def __getitem__(self, idx): img_path = os.path.join(self.img_dir, self.img_labels.iloc[idx, 0]) image = read_image(img_path) label = self.img_labels.iloc[idx, 1] if self.transform: image = self.transform(image) if self.target_transform: label = self.target_transform(label) return image, label 1.2 图像（文件夹名为label 形式）12345data_dir = &#x27;data/hymenoptera_data&#x27;train_datasets = datasets.ImageFolder( os.path.join(data_dir, &quot;train&quot;) ,data_transforms[&quot;train&quot;])val_datasets = datasets.ImageFolder( os.path.join(data_dir, &quot;val&quot;) ,data_transforms[&quot;val&quot;]) 1.3 图像（在线拉取）12345train_data = datasets.MNIST(root=&#x27;data&#x27;, train=True, download=True, transform=data_transforms[&quot;train&quot;], target_transform=xxx) 1.4 图片转换1234567891011121314151617# input变换data_transforms = transforms.Compose([ transforms.RandomResizedCrop(224), transforms.RandomHorizontalFlip(), transforms.ToTensor(), transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]) ])# RandomResizedCrop: 随机长宽比裁剪# RandomHorizontalFlip: 随机水平翻转# RandomVerticalFlip: 随机垂直翻转# ToTensor: 转换为tensor# Normalize: 像素值进行归一化处理# target 变换# 把整数变成一个one-hot的tensortarget_transform = Lambda(lambda y: torch.zeros(10, dtype=torch.float) \\ .scatter_(dim=0, index=torch.tensor(y), value=1)) 2 DataLoader123456train_dataloader = torch.utils.data.DataLoader(train_data, batch_size=64, shuffle=True, num_workers=4)for batch, (X, y) in enumerate(train_dataloader): X, y = X.to(device), y.to(device)","categories":[{"name":"Pytorch基础","slug":"Pytorch基础","permalink":"http://example.com/categories/Pytorch%E5%9F%BA%E7%A1%80/"}],"tags":[{"name":"Pytorch基础","slug":"Pytorch基础","permalink":"http://example.com/tags/Pytorch%E5%9F%BA%E7%A1%80/"}]},{"title":"2-tensor基础","slug":"pytorch基础/2-tensor操作","date":"2023-07-24T09:18:03.276Z","updated":"2023-07-25T06:26:52.819Z","comments":true,"path":"2023/07/24/pytorch基础/2-tensor操作/","link":"","permalink":"http://example.com/2023/07/24/pytorch%E5%9F%BA%E7%A1%80/2-tensor%E6%93%8D%E4%BD%9C/","excerpt":"","text":"1 tensor属性1.1 tensor初始化1234shape = (2,3,)rand_tensor = torch.rand(shape)ones_tensor = torch.ones(shape)zeros_tensor = torch.zeros(shape) 1.2 tensor性质1234tensor = torch.rand(3,4)print(f&quot;Shape of tensor: &#123;tensor.shape&#125;&quot;)print(f&quot;Datatype of tensor: &#123;tensor.dtype&#125;&quot;)print(f&quot;Device tensor is stored on: &#123;tensor.device&#125;&quot;) 2 对象转换2.1 list – tensor123456data = [[1, 2], [3, 4]]# list--&gt; tensor# 数据类型是自动推断出来x_tensor = torch.tensor(data)# tensor--&gt; listx_list = x_tensor.tolist() 2.2 numpy – tensor12345np_array = np.array(data)# numpy --&gt; tensorx_tensor = torch.from_numpy(np_array)# tensor --&gt; numpyx_numpy = x_tensor.numpy() 2.3 tensor – tensor12x_ones = torch.ones_like(x_data) # retains the properties of x_datax_rand = torch.rand_like(x_data, dtype=torch.float) # overrides the datatype of x_data 3 运算&amp;操作3.1 矩阵乘法1234y1 = tensor @ tensor.Ty2 = tensor.matmul(tensor.T)y3 = torch.rand_like(tensor)torch.matmul(tensor, tensor.T, out=y3) 3.2 矩阵点乘12345z1 = tensor * tensorz2 = tensor.mul(tensor)z3 = torch.rand_like(tensor)torch.mul(tensor, tensor, out=z3) 3.3 矩阵拼接1234# 指定的dim 数量增加，除了dim之外的dim需要相同才行t1 = torch.cat([tensor, tensor, tensor], dim=1)# 两个要进行stack的tensor的dim数量应该相同，stack操作之后得到的结果会多出一维，即dim的数量会+1。t1 = torch.stack([tensor, tensor, tensor], dim=1) 3.4 矩阵升降维123456# 升维。插入指定维度，值为1tensor.unsqueeze(dim=0)# 降维。压缩指定维度，该维度值必须为1# 当dim不指定时，压缩所有维度值为1的维tensor.squeeze(dim=0)tensor.squeeze()","categories":[{"name":"Pytorch基础","slug":"Pytorch基础","permalink":"http://example.com/categories/Pytorch%E5%9F%BA%E7%A1%80/"}],"tags":[{"name":"Pytorch基础","slug":"Pytorch基础","permalink":"http://example.com/tags/Pytorch%E5%9F%BA%E7%A1%80/"}]},{"title":"1-pytorch快速开始","slug":"pytorch基础/1-pytorch快速开始","date":"2023-07-24T09:15:16.547Z","updated":"2023-07-24T10:01:37.638Z","comments":true,"path":"2023/07/24/pytorch基础/1-pytorch快速开始/","link":"","permalink":"http://example.com/2023/07/24/pytorch%E5%9F%BA%E7%A1%80/1-pytorch%E5%BF%AB%E9%80%9F%E5%BC%80%E5%A7%8B/","excerpt":"","text":"0 导库123456import torchfrom torch import nnfrom torch.utils.data import DataLoaderfrom torchvision import datasetsfrom torchvision.transforms import ToTensor, Lambda, Composeimport matplotlib.pyplot as plt 1 数据准备torchvision.datasets模块包含了许多真实世界的视觉数据的数据集对象，如CIFAR、COCO。以下通过datasets在线加载FashionMNIST数据集。 123456789101112131415# Download training data from open datasets.training_data = datasets.FashionMNIST( root=&quot;data&quot;, train=True, download=True, transform=ToTensor(),)# Download test data from open datasets.test_data = datasets.FashionMNIST( root=&quot;data&quot;, train=False, download=True, transform=ToTensor(),) 将dataset装载入DataLoader中，DataLoader可认为是一个数据迭代器，其支持数据的自动批处理、采样、洗牌和多进程数据加载。这里定义一个 batch&#x3D;64，即dataloader可迭代的每个元素将返回一个批次，包括64个元素的特征和标签。 123456789batch_size = 64# Create data loaders.train_dataloader = DataLoader(training_data, batch_size=batch_size)test_dataloader = DataLoader(test_data, batch_size=batch_size)for X, y in test_dataloader: print(&quot;Shape of X [N, C, H, W]: &quot;, X.shape) print(&quot;Shape of y: &quot;, y.shape, y.dtype) break 2 创建模型为了在PyTorch中定义一个神经网络，我们创建一个继承自nn.Module的类。我们在__init__函数中定义网络的层，并在forward函数中指定数据将如何通过网络。为了加速神经网络的操作，如果有GPU的话，我们把它移到GPU上。 1234567891011121314151617181920212223# Get cpu or gpu device for training.device = &quot;cuda:0&quot; if torch.cuda.is_available() else &quot;cpu&quot;# Define modelclass NeuralNetwork(nn.Module): def __init__(self): super(NeuralNetwork, self).__init__() self.flatten = nn.Flatten() self.linear_relu_stack = nn.Sequential( nn.Linear(28*28, 512), nn.ReLU(), nn.Linear(512, 512), nn.ReLU(), nn.Linear(512, 10) ) def forward(self, x): x = self.flatten(x) logits = self.linear_relu_stack(x) return logitsmodel = NeuralNetwork().to(device)print(model) 3 优化模型参数3.1 模型训练函数12345678910111213141516171819202122loss_fn = nn.CrossEntropyLoss()optimizer = torch.optim.SGD(model.parameters(), lr=1e-3)def train(dataloader, model, loss_fn, optimizer): size = len(dataloader.dataset) model.train() for batch, (X, y) in enumerate(dataloader): X, y = X.to(device), y.to(device) # Compute prediction error pred = model(X) loss = loss_fn(pred, y) # Backpropagation optimizer.zero_grad() loss.backward() optimizer.step() if batch % 100 == 0: loss, current = loss.item(), batch * len(X) print(f&quot;loss: &#123;loss:&gt;7f&#125; [&#123;current:&gt;5d&#125;/&#123;size:&gt;5d&#125;]&quot;) 3.2 模型评估函数1234567891011121314def test(dataloader, model, loss_fn): size = len(dataloader.dataset) num_batches = len(dataloader) model.eval() test_loss, correct = 0, 0 with torch.no_grad(): for X, y in dataloader: X, y = X.to(device), y.to(device) pred = model(X) test_loss += loss_fn(pred, y).item() correct += (pred.argmax(1) == y).type(torch.float).sum().item() test_loss /= num_batches correct /= size print(f&quot;Test Error: \\n Accuracy: &#123;(100*correct):&gt;0.1f&#125;%, Avg loss: &#123;test_loss:&gt;8f&#125; \\n&quot;)1 3.3 启动训练&amp;评估123456epochs = 5for t in range(epochs): print(f&quot;Epoch &#123;t+1&#125;\\n-------------------------------&quot;) train(train_dataloader, model, loss_fn, optimizer) test(test_dataloader, model, loss_fn)print(&quot;Done!&quot;) 4 保存模型12torch.save(model.state_dict(), &quot;model.pth&quot;)print(&quot;Saved PyTorch Model State to model.pth&quot;) 5 加载模型12model = NeuralNetwork()model.load_state_dict(torch.load(&quot;model.pth&quot;)) 6 模型推理12345678910111213141516171819classes = [ &quot;T-shirt/top&quot;, &quot;Trouser&quot;, &quot;Pullover&quot;, &quot;Dress&quot;, &quot;Coat&quot;, &quot;Sandal&quot;, &quot;Shirt&quot;, &quot;Sneaker&quot;, &quot;Bag&quot;, &quot;Ankle boot&quot;,]model.eval()x, y = test_data[0][0], test_data[0][1]with torch.no_grad(): pred = model(x) predicted, actual = classes[pred[0].argmax(0)], classes[y] print(f&#x27;Predicted: &quot;&#123;predicted&#125;&quot;, Actual: &quot;&#123;actual&#125;&quot;&#x27;)","categories":[{"name":"Pytorch基础","slug":"Pytorch基础","permalink":"http://example.com/categories/Pytorch%E5%9F%BA%E7%A1%80/"}],"tags":[{"name":"Pytorch基础","slug":"Pytorch基础","permalink":"http://example.com/tags/Pytorch%E5%9F%BA%E7%A1%80/"}]},{"title":"Hello World","slug":"hello-world","date":"2023-07-21T10:28:18.087Z","updated":"2023-07-23T10:23:20.541Z","comments":true,"path":"2023/07/21/hello-world/","link":"","permalink":"http://example.com/2023/07/21/hello-world/","excerpt":"","text":"今天坑坑洼洼可算是把网站给搭好了，看着简洁漂亮的首页，想着以后就有专属自己的网站，就很开心。背景图是《权游》里面的某一幕的简画，守夜人面对夜鬼的来袭，背水一战，颇为壮观。 为什么要做个人博客呢？说实话，没有特别的理由。或许是出于新鲜感；或许是工作太无聊；或许是外界太嘈杂，想在数字世界中寻找一片净土；或许是想把有趣的、新奇的东西系统地放进来，有一天可以带朋友来参观，看！这是我曾经的快乐和珍藏。 现实世界有太多约束，说话做事写文章处处存在隐形的规矩，这些规矩容易消磨本身藏在事物的乐趣。我想，在这里就少点规矩吧，说说废话，吹吹牛逼又怎么样呢？有时候写点生活感悟也不怕别人说我假正经。嗯，没错，这是我的展厅！当然，对于涉及实操性或理论性的文章，行文还是遵从逻辑，便于理解和回顾。 那么，第一篇写点啥呢，emmm…","categories":[{"name":"建站","slug":"建站","permalink":"http://example.com/categories/%E5%BB%BA%E7%AB%99/"}],"tags":[{"name":"建站","slug":"建站","permalink":"http://example.com/tags/%E5%BB%BA%E7%AB%99/"}]}],"categories":[{"name":"Pytorch基础","slug":"Pytorch基础","permalink":"http://example.com/categories/Pytorch%E5%9F%BA%E7%A1%80/"},{"name":"建站","slug":"建站","permalink":"http://example.com/categories/%E5%BB%BA%E7%AB%99/"}],"tags":[{"name":"Pytorch基础","slug":"Pytorch基础","permalink":"http://example.com/tags/Pytorch%E5%9F%BA%E7%A1%80/"},{"name":"建站","slug":"建站","permalink":"http://example.com/tags/%E5%BB%BA%E7%AB%99/"}]}